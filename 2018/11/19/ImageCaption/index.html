<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="图像标注,">










<meta name="description" content="对Image Caption的一系列胡扯">
<meta name="keywords" content="图像标注">
<meta property="og:type" content="article">
<meta property="og:title" content="Image Caption">
<meta property="og:url" content="http://yoursite.com/2018/11/19/ImageCaption/index.html">
<meta property="og:site_name" content="Goodbye World">
<meta property="og:description" content="对Image Caption的一系列胡扯">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/2.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/3.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/4.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/5.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/6.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/7.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/8.png">
<meta property="og:image" content="http://yoursite.com/Library/Application%20Support/typora-user-images/image-20181111144138283.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/9.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/10.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/11.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/12.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/13.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/14.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/15.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/16.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/17.png">
<meta property="og:image" content="http://yoursite.com/2018/11/19/ImageCaption/18.png">
<meta property="og:updated_time" content="2018-12-04T03:29:20.432Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Image Caption">
<meta name="twitter:description" content="对Image Caption的一系列胡扯">
<meta name="twitter:image" content="http://yoursite.com/2018/11/19/ImageCaption/1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/11/19/ImageCaption/">






  <title>Image Caption | Goodbye World</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Goodbye World</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/19/ImageCaption/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Citehtap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/panda.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Goodbye World">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Image Caption</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-19T22:46:44+08:00">
                2018-11-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Image-Caption/" itemprop="url" rel="index">
                    <span itemprop="name">Image Caption</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>对Image Caption的一系列胡扯</p>
<a id="more"></a>
<h2 id="0-总结"><a href="#0-总结" class="headerlink" title="0.总结"></a>0.总结</h2><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>Image Caption(图像标注、看图说话)的主要任务是对所给定的图片，由系统自动给出一句描述图片内容的一句话(通常为英文)。与传统的计算机视觉任务相比，不仅要捕捉目标对象，还需要用自然语言描述目标之间的内在联系(目标的属性、目标涉及到的动作以及目标间的关系)。</p>
<h3 id="研究思路"><a href="#研究思路" class="headerlink" title="研究思路"></a>研究思路</h3><h4 id="1-基于LSTM的方法"><a href="#1-基于LSTM的方法" class="headerlink" title="1. 基于LSTM的方法"></a>1. 基于LSTM的方法</h4><p>基于LSTM的方法最初来自NLP的Encoder-Decoder模型：在论文<em>Show and Tell: A Neural Image Caption Generator</em>中，将Encoder部分用CNN实现，将图片通过CNN转换图像特征，作为Decoder的输入向量。</p>
<p>在上述的Show and Tell基础上，有很多改进的思路：</p>
<ul>
<li>论文 <em>What Value Do Explicit High Level Concepts Have in Vision to Language Problems?</em> 认为，Encoder部分的CNN的最终分类层中蕴含了大量关于图片内容信息；</li>
<li>论文 <em>Rethinking the Form of Latent States in Image Captioning</em> 认为，Decoder部分的RNN最终将图片信息转化为一维向量的做法并不自然，因此尝试将图片信息转化为二维矩阵；</li>
<li>论文 <em>Unpaired Image Captioning by Language Pivoting</em> 针对一些缺少数据集的语言，提出利用支点语言来进行图像标注的方法；</li>
<li>论文 <em>“Factual” or “Emotional”: Stylized Image Captioning with Adaptive Learning and Attention</em> 实现了一种能生成程式化图像描述的方法；</li>
<li>论文 <em>Exploring Visual Relationship for Image Captioning</em> 认为，关注图像中对象之间的关系可以更好地生成图像描述；</li>
<li>论文 <em>Recurrent Fusion Network for Image Captioning</em> 认为，在Encoder模块加入额外的CNN可以让模型更全面地理解图像内容；</li>
<li>论文 <em>Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</em> 提出在模型中加入自我检索模块，避免模型生成的描述只是常用词句的重复。</li>
</ul>
<h4 id="2-基于CNN的方法"><a href="#2-基于CNN的方法" class="headerlink" title="2. 基于CNN的方法"></a>2. 基于CNN的方法</h4><p>基于CNN的方法也运用了Encoder-Decoder模型，但本方法用CNN代替基于LSTM的方法中的LSTM模块，并取得了与之相近的图像标注性能。虽然在实验中，本方法的损失都要比基于LSTM的方法的损失高，效果不如基于LSTM的方法。但是本方法在相同的时间内，可以训练更多的参数。因此，可能在一些不强求准确率的、希望能快速实现的任务中，更适合采用基于CNN的方法。</p>
<h4 id="3-注意力机制"><a href="#3-注意力机制" class="headerlink" title="3.注意力机制"></a>3.注意力机制</h4><p>注意力机制并不是一个独立的研究思路，而是一种对人类的注意力机制的借鉴。</p>
<ul>
<li>论文<em>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</em>在Show and Tell的基础上加入了注意力机制，让模型能有选择性地关注图像的某些部分；</li>
<li>论文<em>Boosted Attention: Leveraging Human Attention for Image Captioning</em>提出了两种不同类型的注意力机制，结合提出了强化注意力机制Boosted Attention。</li>
</ul>
<h3 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h3><p>上述论文中使用到的数据集主要有：</p>
<ul>
<li><a href="http://cocodataset.org/" target="_blank" rel="noopener">microsoft COCO</a></li>
<li><a href="https://forms.illinois.edu/sec/1713398" target="_blank" rel="noopener">flicker 8k</a></li>
<li><a href="http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/" target="_blank" rel="noopener">flicker 30k</a></li>
</ul>
<h3 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h3><ul>
<li>BLEU是最常用的评价标准，BLEU实质是计算两个句子的共现词频率，但容易陷入常用词的陷阱中；</li>
<li>CIDEr，B-N，M，R；</li>
<li>论文<em>Learning to Evaluate Image Captioning</em>提出了一个既能区分人与机器产生的描述，又能良好地应对错误样例的评价标准。</li>
</ul>
<h3 id="下一阶段的学习"><a href="#下一阶段的学习" class="headerlink" title="下一阶段的学习"></a>下一阶段的学习</h3><ol>
<li>Encoder-Decoder：目前我对Encoder-Decoder只有比较直观的了解，还需要更进一步的理解；</li>
<li>CNN、RNN及其变种：CNN在Encoder部分使用广泛，而且对CNN的选择(VGG、GoogleNet等)、CNN的训练都需要一些技巧；RNN及其变种在Decoder部分使用。</li>
</ol>
<hr>
<h2 id="1-Show-and-Tell-A-Neural-Image-Caption-Generator"><a href="#1-Show-and-Tell-A-Neural-Image-Caption-Generator" class="headerlink" title="1. Show and Tell: A Neural Image Caption Generator"></a>1. Show and Tell: A Neural Image Caption Generator</h2><p>在<em>Show and Tell: A Neural Image Caption Generator</em>中，利用了在机器翻译中常用的Encoder-Decoder结构实现图像标注任务。</p>
<p>代码地址：<a href="https://github.com/jazzsaxmafia/show_and_tell.tensorflow" target="_blank" rel="noopener">https://github.com/jazzsaxmafia/show_and_tell.tensorflow</a></p>
<p>主要研究人员主页：<a href="http://www1.icsi.berkeley.edu/~vinyals/" target="_blank" rel="noopener">http://www1.icsi.berkeley.edu/~vinyals/</a></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><blockquote>
<p>Encoder-Decoder结构: 在机器翻译中，将原单词序列 <script type="math/tex">x_1,\dots ,x_n</script> 通过Encoder部分“编码”为一个向量表示，再利用Decoder部分对此向量进行”解码”，输出目标单词序列 <script type="math/tex">y_1,\dots,y_m</script> 。</p>
</blockquote>
<p>本论文的方法是，将原来Encoder-Decoder结构中的Encoder部分替换成CNN，利用在图像中常用的CNN来提取图像的特征，再通过Decoder解码生成标注。结构如下图所示：</p>
<p><img src="1.jpg" alt="image-20181109224331812"></p>
<blockquote>
<p>先看提取图像特征的CNN部分，由于这篇论文是谷歌出品的，因此这部分就使用了自家的Inception模型。再看Decoder部分，将RNN换成了性能更好的LSTM，输入还是word embedding，每步的输出是单词表中所有单词的概率，这些都是标准做法了，就不再赘述。</p>
</blockquote>
<p>模型的参数为 $\theta$ ，对于一张(训练集中的)图片 $I$ ，其正确的描述为序列 $S=\{S_1,S_2,\dots\}$  ，那么我们要通过训练最大化概率 $p(S|I;\theta)$ ，也就是最大化<strong>对给定图片生成正确描述</strong>的概率，用以下公式表示：</p>
<script type="math/tex; mode=display">
\theta^*=\arg \max_\theta \sum_{(I,S)} \log p(S|I;\theta)</script><p>由于 $S$ 可以表示任何句子，而句子的长度是无限制的，因此常用链规则来描述在 $S_0,\dots,S_N$ 上的联合概率，其中 $N$ 是特定样本的长度：</p>
<script type="math/tex; mode=display">
\log p(S|I;\theta)=\sum_{t=0}^{N}\log p(S_t|I,S_0,\dots,S_{t-1})</script><p>因为RNN会出现梯度消失和爆炸(vanishing and exploding gradients)，所以使用RNN的改进版LSTM。LSTM的主要结构为：</p>
<blockquote>
<ul>
<li><p>LSTM通过输入门、遗忘门和输出门来实现信息的控制</p>
</li>
<li><p>遗忘门：该门决定丢弃信息的多少，会读取 $h_{t-1},x_t$ ，给每个在细胞状态 $C_{t-1}$ 的数字输出一个0到1的数值，表示对信息的保留程度：</p>
<script type="math/tex; mode=display">
f_t=\sigma(W_f·[h_{t-1},x_t]+b_f)</script><p>其中 $h_{t−1}$ 表示的是上一个cell的输出， $x_t$ 表示的是当前细胞的输入， $σ$ 表示sigmod函数。</p>
</li>
<li><p>输入门：该门决定新信息的加入量。首先，由一个sigmoid层决定哪些信息需要更新；由一个tanh层生成一个向量，也就是备选的用来更新的内容 $\tilde{C_t}$ 。在下一步，我们把这两部分联合起来，对cell的状态进行一个更新。最后把旧状态与 $f_t$ 相乘，丢弃掉需要丢弃的信息，再加上 $i_t*\tilde{C_t}$ ，这就是新的候选值：</p>
<script type="math/tex; mode=display">
i_t=\sigma(W_i·[h_{t-1},x_t]+b_i) \\
\tilde{C_t}=\tanh(W_C·[h_{t-1},x_t]+b_C) \\
C_t=f_t*C_{t-1}+i_t*\tilde{C_t}</script></li>
<li><p>输出门：该门决定输出的值。首先，通过一个sigmoid层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分：</p>
<script type="math/tex; mode=display">
o_t=\sigma(W_o·[h_{t-1},x_t]+b_o) \\
h_t=o_t*\tanh(C_t)</script></li>
</ul>
</blockquote>
<p>模型的损失函数是每一步中正确单词的负对数概率的总和：</p>
<script type="math/tex; mode=display">
L(I,S)=-\sum_{t=1}^N\log{p_t(S_t)}</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>结果评价方式为人工评价，按照<em>Framing image description as a ranking task: Data, models and evaluation metrics</em>中的要求对生成的句子评分。</p>
<p><img src="2.png" alt="QQ20181118-111054@2x"></p>
<h3 id="问题与挑战"><a href="#问题与挑战" class="headerlink" title="问题与挑战"></a>问题与挑战</h3><p>本论文中提到，该模型存在过拟合的问题，解决方法为：</p>
<ol>
<li>对我们模型的CNN部分的权重初始化为预处理模型(如ImageNet)</li>
<li>保持 $W_e$ 的未初始化状态</li>
<li>使用dropout以及使用集成模型避免过拟合</li>
</ol>
<hr>
<h2 id="Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention"><a href="#Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention" class="headerlink" title="Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"></a>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</h2><p>代码地址：<a href="https://github.com/yunjey/show-attend-and-tell" target="_blank" rel="noopener">https://github.com/yunjey/show-attend-and-tell</a></p>
<p>主要研究人员主页：<a href="http://kelvinxu.github.io/" target="_blank" rel="noopener">http://kelvinxu.github.io/</a></p>
<p>本论文是对第一篇论文的改进，在Show and Tell的基础上，论文<em>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</em>加入了attention机制。注意力机制模拟了人的注意力，即：在人的视觉系统中，有注意力的存在，因此人在处理图像的时候，没有把图片当作一个整体静态的图片，而是跟随一些显著的特征，按照需要动态地浮现图片的某些部分。本文的主要贡献为：</p>
<ul>
<li>介绍了基于含两种attention：硬注意力机制和软注意力机制的通用框架。软确定性注意力机制通过标准的反向传播算法可进行训练；硬随机性注意力机制通过最大化一个近似可变的下界(等效于REINFORCE)可进行训练(<em>maximizing an approximate variational lower bound or equivalently by</em>)</li>
<li>通过显示注意力集中在“哪里”和“什么”，来演示解读该框架的结果的方式。</li>
<li>在三个标准数据集上定量地验证了注意力的有用性且拥有最好的性能。</li>
</ul>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><ol>
<li><p>Encoder：卷积特征</p>
<p>通过CNN，从图片中提取特征 $a$ ( $a$ 也被称为annotation向量)：</p>
<script type="math/tex; mode=display">
a=\{\mathbf a_1,\dots,\mathbf a_L\},\mathbf a_i\in \mathbf R^D \\</script><p>其中特征 $a$ 共有 $L$ 个，每个都是一个 $D$ 维向量。先前的<em>Show and Tell</em>中通过CNN只提取了一个固定长度的特征，而在本文中，提取的特征代表了图像 $L$ 个不同的位置的特征。</p>
<p>为了能在特征向量与2维图像的一部分之间有更好的相似度，本模型使用<strong>lower convolutional layer</strong>，而不是像先前的工作中那样使用全连接层。这样可以让Decoder通过在 $L$ 个特征中选择某个子集，以此将注意力集中在图像的某个部分上——这就是注意力机制。</p>
</li>
<li><p>Decoder：LSTM网络</p>
<p>LSTM是一个RNN cell的变种：</p>
<p><img src="3.png" alt="image-20181109231343658"></p>
<p>具体的计算公式为：</p>
<p><img src="4.png" alt="image-20181110122326635"></p>
</li>
</ol>
<p>   其中 $i_t,f_t,c_t,o_t,h_t$ 分表代表输入、遗忘、记忆、输出和LSTM的隐藏层状态。 $\mathbf y_{t-1}$ 代表上一阶段的输出，而 $\mathbf E_ {\mathbf y_{t-1}}$ 代表输出对应的word embedding形式。</p>
<p>   Decoder部分在时间 $t$ 时根据上下文向量 $\hat z_{t}$ 与上一阶段的隐藏层状态 $h_{t-1}$ 。其中上下文向量 $\hat z_{t}$ 是一个与特定输入区域相关的视觉信息，可通过图片的特征进行计算：</p>
<script type="math/tex; mode=display">
\hat z_{t}=\phi(\{\mathbf a_i,\alpha_i\})</script><p>   其中， $\phi$ 代表一个接受一组向量及其权重，返回单个向量的函数； $\alpha_{t,i}$ (时间 $t$ 时的 $\alpha_i$ )代表生成第 $t$ 个词的时候，位置 $i$ 的特征所占的权重， $\alpha$ 是一个关于上一阶段的隐藏层状态 $h_{t-1}$ 和位置 $i$ 的特征 $a_i$ 的函数： </p>
<script type="math/tex; mode=display">
   e_{ti}=f_{att}(a_i,h_{t-1}) \\
   \alpha_{ti}=\frac{\exp(e_{ti})}{\sum_{k=1}^L\exp(e_tk)}</script><h3 id="软与硬"><a href="#软与硬" class="headerlink" title="软与硬"></a>软与硬</h3><p>文中介绍了两种注意力机制Stochastic “Hard” Attention和Deterministic “Soft” Attention。其中软注意力使用比较广泛，但两种注意力机制都能提升模型的性能。我目前理解的两种机制的区别主要在对 $\alpha$ 的解读上：</p>
<ol>
<li><p>Stochastic “Hard” Attention</p>
<p>当模型生成第 $t$ 个词的时候，我们用 $s_t$ 来表示模型决定注意的位置。 $s_{t,i}$ 是一个独热码的变量(an indicator one-hot variable)，它的第 $i$ 个值为1，其他都为0（也就是说模型在时间 $t$ ，注意了第 $i$ 个位置的信息）。这里的 $\alpha_{t,i}$ 表示“<strong>位置 $i$ 是为了生成下一个单词而要关注的正确位置</strong>”的概率。</p>
</li>
<li><p>Deterministic “Soft” Attention</p>
<p>在软注意力中， $\alpha$ 表示<strong>相对重要性</strong>，可通过与 $a_i$ 进行运算得到：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{p(s_t|a)}[\hat{\mathbf{z}_t}]=\sum_{i=1}^L\alpha_{t,i}\mathbf{a}_i</script></li>
</ol>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><ul>
<li>两种注意力模型都通过SGD来训练(需要使用合适的学习率算法)。本文提到：对Flickr8k数据集，使用RMSProp效果最佳；对于Flickr30k/MS COCO数据集，使用了Adam算法(效果论文中未提及)。</li>
<li>为创建decoder可用的向量 $a_i$ ，使用VGGnet在ImageNet上进行预训练且不进行微调（原则上任何encoding方法都可以使用），论文中在第四个卷积层前的池化层使用了 14 x 14 x 512 的特征映射，这意味着decoder以 196 x 512 的尺寸进行操作</li>
<li>使用BLEU度量标准</li>
<li>在Flickr8k上应用soft attention时，也使用了Whetlab(<a href="https://www.whetlab.com/" target="_blank" rel="noopener">https://www.whetlab.com/</a> ，链接无法打开)</li>
</ul>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p><img src="5.png" alt="image-20181106191857266"></p>
<h3 id="改进与挑战"><a href="#改进与挑战" class="headerlink" title="改进与挑战"></a>改进与挑战</h3><ol>
<li>卷积特征提取器的选择：GoogLeNet/VGG/AlexNet</li>
<li>单个模型 vs. 集成模型（本论文适用单个模型）</li>
<li>数据集的分句。本文使用了Flicker8k的预定义的分句，但是Flicker30k和COCO缺少标准化的分句</li>
</ol>
<hr>
<h2 id="3-Convolutional-Image-Captioning"><a href="#3-Convolutional-Image-Captioning" class="headerlink" title="3. Convolutional Image Captioning"></a>3. Convolutional Image Captioning</h2><p>主要研究人员主页：<a href="https://github.com/jyotianeja" target="_blank" rel="noopener">https://github.com/jyotianeja</a>  不过没有太多相关信息</p>
<p>第三篇论文也是在第一篇论文之后发表的，作者认为，使用LSTM进行图像标注有很多弊端：</p>
<ol>
<li>由于LSTM具有复杂的寻址(address)、重写(overwrite)机制以及内在的序列处理，再加上基于时间的反向传播，在模型的训练期间需要大量的存储。</li>
<li>与非连续的CNNs(non-sequential CNNs)相比，LSTM在面对全新的问题时需要更多的工程。</li>
</ol>
<p>因此，本论文<em>Convolutional Image Captioning</em>根据最近卷积体系结构在其他序列到序列的任务（条件图像生成，机器翻译等）上的成功，提出了解决图像标注这一视觉-语言问题的卷积体系结构，并且引入了注意力机制。本论文中的方法的效果可以与基于LSTM的方法相媲美。</p>
<h3 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h3><p>本论文使用了基于CNN的方法解决图像标注问题，模型如下图所示：</p>
<p><img src="6.png" alt="image-20181110173514754"></p>
<p>可以发现，本文中的模型与RNN有一点类似，第一个和最后一个部分都是输入/输出词嵌入向量(WordEmbedding)。不过，在RNN方法中，中间部分使用LSTM或者GRU；但在本论文使用的基于CNN的方法中，使用了masked convolutions。和RNN中的中间部分不同，这一部分是不带有递归函数的前馈(feed-forward without any recurrent function)。</p>
<p>本论文使用一个简单的前馈深度网络 $f_w$ 来model $p_{i,w}(y_i)$ ：</p>
<script type="math/tex; mode=display">
p_{i,w}(y_i|y_{<i},I)=f_w(y_i,y_{<i},I)</script><p>也就是根据先前的单词 <script type="math/tex">y_{<i}</script> 来预测单词 <script type="math/tex">y_i</script> 。具体过程可以大致描述为：首先从起始符号 <script type="math/tex"><S></script> 开始，通过一个前馈网络生成 <script type="math/tex">p_{1,w}(y_1|\emptyset,I)</script> ，这样 <script type="math/tex">y_1\sim p_{1,w}(y_1|\emptyset,I)</script> 就被采样了；随后 <script type="math/tex">y_1</script> 会被反馈到前馈网络中来生成 <script type="math/tex">y_2</script> 。重复上述步骤就可以生成全部标注了。</p>
<h3 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h3><p>本论文中的CNN方法的体系结构如下图所示：</p>
<p><img src="7.png" alt="image-20181110181038217"></p>
<p><strong>训练</strong>：在训练过程中，我们输入文字序列并在前后分别添加开始符号 <script type="math/tex"><S></script> 和结束符号 <script type="math/tex"><E></script> ，如： <script type="math/tex">\{<S>,y_1^*,\dots,y_5^*,<E>\}</script> 。这些文字通过一下流程进行处理：</p>
<ol>
<li>通过一个输入嵌入层</li>
<li>与图像嵌入相结合</li>
<li>用CNN模块处理</li>
<li>输出嵌入层产生输出概率分布</li>
</ol>
<p><strong>输入嵌入</strong>：与RNN/LSTM相通，本论文的方法训练了一个嵌入层，嵌入层通过被one-hot编码后的输入词进行训练。输入嵌入与图像嵌入相结合，在后面会作为前馈CNN模块的输入。</p>
<p><strong>图像嵌入</strong>：图像 $I$ 的特征是从VGG16的fc7层上获取的。其中VGG16网络在ImageNet数据集上进行了预训练。在实践中，在fc7上应用了dropout与ReLU，并利用线性层来获得512维的嵌入。</p>
<p><strong>CNN模块</strong>：CNN模块对输入嵌入与图像嵌入的组合进行操作，它使用了3层masked convolutions，随后CNN模块会输出文字，每个文字都是512维向量。</p>
<p><strong>分类层</strong>：本方法使用一个线性层，来将CNN模块输出的512维的编码为每个单词的256维表示，然后通过一个全连接层，将其降采样为 $|y|$ 维的激活，最后通过softmax获得输出词概率 $p_{i,w}(y_i|y_{&lt;i},I)$ 。</p>
<p><strong>训练</strong>：我们用概率 $p_{i,w}(y_i|y_{&lt;i},I)$ 的交叉熵损失来训练CNN模块与嵌入层。</p>
<h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>本方法同样引入了注意力机制——生成512维的图像向量并将其加入每一层的字嵌入中。正式的表述为：</p>
<p>用 d_j 表示CNN模块中单词 j 的嵌入； W 表示应用在 d_j 上的线性层的参数矩阵； c_i 表示位置 i 的512维的空间conv-5特征； $a_{i,j}$ 表示注意力参数：</p>
<script type="math/tex; mode=display">
a_{i,j}=\frac{\exp{(W(d_j)^Tc_i)}}{\sum_i\exp{(W(d_j)^Tc_i)}}</script><p>那么单词 $i$ 的图像向量可通过 $\sum_i a_{ij}c_i$ 计算得到。</p>
<h3 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h3><ol>
<li><p>训练时间</p>
<p><img src="8.png" alt="image-20181111141814830"></p>
<p>从上表为LSTM方法、本文的CNN方法、本文的CNN方法加上注意力机制的参数以及训练时间数据。因为CNN不像LSTM那样需要进行顺序处理，因此CNN方法的每个参数训练得都要比LSTM方法要快。本文使用PyTorch实现，使用Nvidia Titan X GPU进行训练。</p>
</li>
<li><p>效果</p>
<p><img src="../../../../../Library/Application%20Support/typora-user-images/image-20181111144138283.png" alt="image-20181111144138283"></p>
<p>图左一：使用的训练集为MSCOCO。从该图可以看出，在训练集和验证集上，CNN方法都比LSTM方法有更高的交叉熵损失</p>
<p>图左二：图中可以看出，本文的CNN方法在在softmax层上有比LSTM方法更高的熵</p>
<p>图右一：虽然CNN方法的训练损失比LSTM方法的高，但是在验证集上，CNN方法的正确率仅比LSTM方法低约1%</p>
</li>
</ol>
<p>因此我觉得可以认为，如果要追求更好的正确率，本论文中的CNN方法并不是非常先进；但CNN方法的优势是可以快速训练——在相同的时间可以训练1.5倍的参数。</p>
<hr>
<h2 id="4-What-Value-Do-Explicit-High-Level-Concepts-Have-in-Vision-to-Language-Problems"><a href="#4-What-Value-Do-Explicit-High-Level-Concepts-Have-in-Vision-to-Language-Problems" class="headerlink" title="4. What Value Do Explicit High Level Concepts Have in Vision to Language Problems?"></a>4. What Value Do Explicit High Level Concepts Have in Vision to Language Problems?</h2><p>主要研究人员主页：<a href="http://www.qi-wu.me/、https://cs.adelaide.edu.au/~chhshen/" target="_blank" rel="noopener">http://www.qi-wu.me/、https://cs.adelaide.edu.au/~chhshen/</a></p>
<p>图像标注的问题与机器语言翻译非常相似。在机器语言翻译中，有很多成果已经显示，即使不关注单词状态的高层模型(higher-level model of state of the word)，也可以得到非常优秀的性能。但本论文认为，在图像标注中常用的模型(也就是<em>show, attend and tell</em>)中，通过Encoder部分的CNN的最终的分类层包含了很多信息，如：图中有无人、图中有无桌子这样的信息。</p>
<h3 id="以属性为基础的V2L模型-An-Attribute-based-V2L-Model"><a href="#以属性为基础的V2L模型-An-Attribute-based-V2L-Model" class="headerlink" title="以属性为基础的V2L模型(An Attribute-based V2L Model)"></a>以属性为基础的V2L模型(An Attribute-based V2L Model)</h3><p>本文提出的V2L模型如下图：</p>
<p><img src="9.png" alt="image-20181112183830152"></p>
<p>这个模型氛围两个部分：图像分析部分和自然语言生成部分。</p>
<p>在图像分析部分：</p>
<ol>
<li>在图像分析部分，通过使用监督学习来预测一个属性集。本论文将这一部分当作一个多分类问题来解决，通过最小化一个element-wise logistic loss function来训练一个相关的CNN</li>
<li>对每个图像 $I$ ，模型都会创建一个固定长度的向量 $V_{att}(I)$ ，该向量的长度与属性集的长度相同。 $V_{att}(I)$ 的每一维都代表相应属性的预测概率</li>
</ol>
<p>在自然语言生成部分：</p>
<p>使用一个基于LSTM的语句生成器，LSTM的输入是属性向量 $V_{att}(I)$ 。对不同的任务，本论文使用了不同的语言模型：</p>
<ul>
<li>对图像标注问题，本论文遵从论文<em>show and tell</em>来通过LSTM生成语句</li>
<li>对单字问题回答(single-word question answering)，遵从论文<em>Image Question Answering: A Visual Semantic Embedding Model and a New Dataset</em>使用LSTM作为一个分类器，由分类器LSTM为每个潜在问题提供一个概率(likelihood)</li>
<li>对可随意回答的问题(open-ended question)，使用一个编码器LSTM来为问题进行编码、另一个解码器LSTM接收属性向量 $V_{att}(I)$ 来生成基于问题的回答。</li>
</ul>
<h3 id="属性预测器"><a href="#属性预测器" class="headerlink" title="属性预测器"></a>属性预测器</h3><p>在本部分，无论最终的任务是什么，首先都要构建一个属性词汇表(attributes vocabulary)。本论文中的词汇表是从训练数据中的描述中提取的，并且不关注时态和单复数形式(如’ride’和’riding’，’bag’与’bags’)。P.S 在后续部分会单独学习得到其他候选词。</p>
<p>有了属性词汇表后，可以根据描述把图像的每一部分与相应的一组词相关联。本论文将这一关联任务当作多标签分类问题( multi-label classification problem)来解决，设计了设计基于区域的多标签分类框架。</p>
<p><img src="10.png" alt="image-20181115121028367"></p>
<p>上图是属性预测网络的结构。使用了AlexNet作为共享CNN(shared CNN)的初始化，然后共享CNN会在多标签数据集下进行微调。假设有 $N$ 个训练样例， $\mathbf{y_i}=[y_{i1},\dots,y_{ic}]$ 是第 $i$ 个图像的标签向量， $y_{ij}=1$ 代表图像与属性 $j$ 相关； $p$ 代表预测概率。那么需要进行最小化的损失函数即为：</p>
<script type="math/tex; mode=display">
J=\frac1N\sum_{i=1}^{N}\sum_{j=1}^c\log (1+\exp(-y_{ij}p_{ij}))</script><h3 id="语言生成器"><a href="#语言生成器" class="headerlink" title="语言生成器"></a>语言生成器</h3><p>与常见做法相似，本论文在此部分通过最大化给定图片描述的概率，训练了一个语言生成模型。与常规做法不同的是，本论文使用<strong>语义属性预测概率(semantic attribute prediction probability) $V_{att}(I)$</strong>作为输入。假设 $\{S_!,\dots,S_L\}$ 是一个文字序列，对给定上下文词和相关图像的单词的对数似然概率可以表示为(The log-likelihood of the words given their context words and the corresponding image can be written as)：</p>
<script type="math/tex; mode=display">
\log p(S|V_{att}(I))=\sum_{t=1}^L\log p(S_t|S_{1:t-1},V_{att}(I))</script><p>其中， $p(S_t|S_{1:t-1},V_{att}(I))$ 代表了在给定属性向量 $V_{att}(I)$ 与其前面的单词 $S_{1:t-1}$ 的情况下，生成单词 $S_t$ 的概率。</p>
<h3 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h3><p>在本论文中，除了图像标注问题，还涉及到了单字问题回答和可随意回答的问题。通过实验说明，在上述的3类问题中，使用本文提出的 $V_{\text{att}}(I) $ 代替其他方法使用的卷积特征，都可以大幅提高模型的性能与效果。</p>
<hr>
<h2 id="5-Learning-to-Evaluate-Image-Captioning"><a href="#5-Learning-to-Evaluate-Image-Captioning" class="headerlink" title="5. Learning to Evaluate Image Captioning"></a>5. Learning to Evaluate Image Captioning</h2><p>主要研究人员主页：<a href="https://www.cs.cornell.edu/~andreas/、http://www.guandaoyang.com/" target="_blank" rel="noopener">https://www.cs.cornell.edu/~andreas/、http://www.guandaoyang.com/</a></p>
<h3 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h3><p>本论文与其他论文不同，没有研究图像标注的问题，而是提出了一个全新的图像标注的<strong>评价指标(evaluation metric)</strong>。本论文提出的评价指标是基于学习的——根据论文的描述：经过训练后，评价指标可以与人一样区分机器生成的描述与人的描述，同时还能很好得应用于<strong>目标错误样例(targeted pathological cases)</strong>。不过，本论文没有考虑到进行人为描述的人的个性——对同一张图片，不同的人也可能给出不同的描述。鉴别这种区别并将其纳入指标中是未来的一个研究方向。</p>
<h3 id="评价指标的评价标准"><a href="#评价指标的评价标准" class="headerlink" title="评价指标的评价标准"></a>评价指标的评价标准</h3><p>目前常用的图像标注模型的评价有BLEU、METEOR、ROUGE和CIDEr，这些指标都有一个共同点，即主要根据生成的描述与参考描述之间的文字重叠程度来判断图像标注的好坏。 最新提出的SPICE标准通过测量生成的描述所构建的场景图与参考描述所构建的场景图的相似性来评价，更接近于人类的判断</p>
<p>这些指标的不足为：</p>
<ol>
<li>许多指标是基于文字重叠程度的，而基于文字重叠程度的指标很难捕获句子的语义，与人类的判断相差甚远。</li>
<li>每个评价指标都有其明显的盲点。比如，基于规则的指标往往不灵活，无法对新的<strong>错误样例(new pathological cases)</strong>作出良好的反应。</li>
</ol>
<p>而本论文提出的指标是基于学习的标注评价指标。这个指标可以直接捕获人类的判断，而且对目标病理案例具有灵活性。并且本论文展示了训练出良好的标注评价的关键因素，并展示了本指标的有效性。</p>
<hr>
<h2 id="6-Rethinking-the-Form-of-Latent-States-in-Image-Captioning"><a href="#6-Rethinking-the-Form-of-Latent-States-in-Image-Captioning" class="headerlink" title="6. Rethinking the Form of Latent States in Image Captioning"></a>6. Rethinking the Form of Latent States in Image Captioning</h2><p>代码地址：<a href="https://github.com/doubledaibo/2dcaption_eccv2018" target="_blank" rel="noopener">https://github.com/doubledaibo/2dcaption_eccv2018</a></p>
<p>主要研究人员主页：<a href="https://sites.google.com/site/daibohr/" target="_blank" rel="noopener">https://sites.google.com/site/daibohr/</a></p>
<h3 id="论文概述-1"><a href="#论文概述-1" class="headerlink" title="论文概述"></a>论文概述</h3><p>本论文对图像标注中的潜在状态(latent states)形式的进行了再思考：先前提到的基于RNN的方法通常都会将潜在状态表示为1维的序列(也就是向量)，并将其认为是正确而自然的做法。本文选择了另外一个办法，即用二维映射(2-D maps)来编码潜在状态。</p>
<h3 id="一维的弊端"><a href="#一维的弊端" class="headerlink" title="一维的弊端"></a>一维的弊端</h3><ul>
<li>为了和潜在状态保持一致，图像也会被压缩为一维向量。这样会损失重要的空间信息、模型变得过于依赖多元词组的统计信息、更容易陷入重复高频率词句的陷阱中；</li>
<li>用一维向量来表示潜在状态，会让潜在状态在解码过程中的动态变化难以进行可视化的分析。</li>
</ul>
<h3 id="二维的优势"><a href="#二维的优势" class="headerlink" title="二维的优势"></a>二维的优势</h3><p>文中的对向量状态表示与二维状态表示进行了研究，表明：</p>
<ol>
<li><p>用二维结构来描述潜在状态，保留了更多的图片信息，可以可视化地分析潜在状态在解码过程中的动态变化：</p>
<p><img src="11.png" alt="image-20181118103443080"></p>
<p>上图显示了几个图像及其生成的图像，以及解码过程后某些通道的激活区域。</p>
</li>
<li><p>使用二维结构能更好地形成标注。在MSCOCO和Flickr30k上都实现了显著的性能提升</p>
</li>
<li><p>二维结构与视觉的解释(visual interpretation)更加相符</p>
</li>
</ol>
<p>经过本文的实验，发现在图像标注问题中，潜在状态的二维状态始终能有更高的性能。并且二维状态还保留了潜在状态的空间局部性，这有助于揭示解码过程的内部动态，并解释视觉和<strong>语言域(linguistic domains)</strong>之间的联系。</p>
<hr>
<h2 id="7-Unpaired-Image-Captioning-by-Language-Pivoting"><a href="#7-Unpaired-Image-Captioning-by-Language-Pivoting" class="headerlink" title="7. Unpaired Image Captioning by Language Pivoting"></a>7. Unpaired Image Captioning by Language Pivoting</h2><p>主要研究人员主页：<a href="http://jxgu.cc/" target="_blank" rel="noopener">http://jxgu.cc/</a></p>
<h3 id="论文概述-2"><a href="#论文概述-2" class="headerlink" title="论文概述"></a>论文概述</h3><p>通常，图像标注会从训练集的文字-图像对中学到从图像到文字序列的映射。但是一些小众语言，很难有大规模的图像标注的语料库。因此本论文提出利用支点语言进行图像标注，也就是希望根据图像生成目标语言的标注，但是没有相应的训练集。因此分别训练图像到支点的映射以及支点语言到目标语言的映射，即 Image Caption Generation + Neural Machine Translation：</p>
<p><img src="12.png" alt="image-20181116195454997"></p>
<p>上图中，虚线表示没有 $i,x,y,\hat{y}$ 分别代表目标语言中的源图像、支点语言句子、目标语言句子、真实的标注。虚线表示两端没有可用的平行语料库；实线表示解码的方向；圆圈内的虚线表示标注与翻译数据之间的风格差异和分布差异。</p>
<blockquote>
<p>和Image caption+谷歌翻译相比优势在哪里呢?</p>
</blockquote>
<hr>
<h2 id="8-Boosted-Attention-Leveraging-Human-Attention-for-Image-Captioning"><a href="#8-Boosted-Attention-Leveraging-Human-Attention-for-Image-Captioning" class="headerlink" title="8. Boosted Attention: Leveraging Human Attention for Image Captioning"></a>8. Boosted Attention: Leveraging Human Attention for Image Captioning</h2><p>主要研究人员主页：<a href="https://scholar.google.com/citations?user=oThw5jEAAAAJ&amp;hl=en" target="_blank" rel="noopener">https://scholar.google.com/citations?user=oThw5jEAAAAJ&amp;hl=en</a> 谷歌学术的主页</p>
<p>​                <a href="https://www-users.cs.umn.edu/~qzhao/" target="_blank" rel="noopener">https://www-users.cs.umn.edu/~qzhao/</a></p>
<h3 id="论文概述-3"><a href="#论文概述-3" class="headerlink" title="论文概述"></a>论文概述</h3><p>因为注意力机制鼓励模型有选择地关注感兴趣的区域，因此它在图像标注任务中应用广泛且有效。现有的模型一般依赖于<strong>自上而下的语言信息(top-down language information)</strong>，并通过优化标注目标来隐式地学习注意力。但是这种方法可能无法在没有直接监督注意力的情况下集中于正确的区域。</p>
<p>为解决这一问题，本文提出了两种注意力机制，认为这两种注意力机制具有互补性质。开发了一个模型Boosted Attention，并将其整合到图像标注模型中。Boosted Attention将<strong>基于刺激的注意力(stimulus-based ttention )</strong>与自上而下的注意力相结合来进行图像标注：</p>
<p><img src="13.png" alt="image-20181118104432465"></p>
<p>上图中，从左到右依次是：原始图片、人的注意力、基于刺激的注意力。</p>
<p>Boosted Attention鼓励模型关注<strong>基于自然语言中的特定任务的自上而下信号的视觉特征(visual features based on task-specific top-down signals from natural language )</strong>，同时关注<strong>由任务独立刺激强调的显著区域(salient regions highlighted by task-independent stimulus)</strong>：</p>
<p><img src="14.png" alt="image-20181118104542442"></p>
<p>上图是Boosted Attention的结构示例，图像通过Encoder产生的图像编码会分别进入自上而下的注意力模块和基于刺激的注意力模块。</p>
<hr>
<h2 id="9-“Factual”-or-“Emotional”-Stylized-Image-Captioning-with-Adaptive-Learning-and-Attention"><a href="#9-“Factual”-or-“Emotional”-Stylized-Image-Captioning-with-Adaptive-Learning-and-Attention" class="headerlink" title="9. “Factual” or “Emotional”: Stylized Image Captioning with Adaptive Learning and Attention"></a>9. “Factual” or “Emotional”: Stylized Image Captioning with Adaptive Learning and Attention</h2><p>主要研究人员主页：<a href="http://www.cs.rochester.edu/u/tchen45/" target="_blank" rel="noopener">http://www.cs.rochester.edu/u/tchen45/</a> 、 <a href="http://www.cs.rochester.edu/u/jluo/" target="_blank" rel="noopener">http://www.cs.rochester.edu/u/jluo/</a></p>
<h3 id="论文概述-4"><a href="#论文概述-4" class="headerlink" title="论文概述"></a>论文概述</h3><p>本论文的图像标注任务比较特殊：希望模型生成具有具有特定语言风格(如：例如，幽默，浪漫，积极、消极)的标注，同时在语义上要准确描述图片内容。为满足这两种要求，本论文提出了一种新颖的<strong>程式化图像标注模型(stylized image captioning model)</strong>。</p>
<p>首先，本文设计了一种LSTM的全新变体：style-factual LSTM。这种LSTM接收会通过两组矩阵分别捕获事实与程式化的信息，并根据先前的上下文来学习两组单词的权重。</p>
<h3 id="style-factual-LSTM"><a href="#style-factual-LSTM" class="headerlink" title="style-factual LSTM"></a>style-factual LSTM</h3><p>style-factual LSTM将两个矩阵组 $S_x,S_h$ 作为权重 $W_x,W_h$ 的对应项来学习程式化的标注；在时间 $t$ ，通过同步学习<strong>自适应权重 (adaptive weights)</strong> $g_{xt},g_{ht}$  来调整 $W_x,S_x$ 和 $W_h,S_h$ 之间的<strong>相对注意力权重(relative attention weights)</strong>。style-factual LSTM的结构如下图所示：</p>
<p><img src="15.png" alt="image-20181116155209519"></p>
<p>上图中，四个权重，$1-g_{ht},1-g_{xt},g_{ht},g_{xt}$ ，分别控制 $W_{hi},W_{xi},S_{hi},S_{xi}$ 矩阵的比例。其中 $W_x,W_h$ 负责根据输入图像生成描述事实的标注，而 $Sx,Sh$ 负责将特定风格融入到标注中。 </p>
<p>在时间 $t$ ，style-factual LSTM会将 $h_{t-1}$ 馈送到具有一个输出节点的两个独立子网络中，其在使用sigmoid单元将输出映射到（0的范围）之后最终计算出gxt和ght。1）。 </p>
<p>直观上讲，当模型侧重于预测描述事实的单词时， $g_{xt}$ 和 $g_{ht}$ 应接近0，这鼓励模型基于 $W_x$ 和 $W_h$ 来预测单词；当模型侧重于预测程式化单词时， $g_{xt}$ 和 $g_{ht}$ 应该接近1，这鼓励模型基于 $Sx$ 和 $S_h$ 来预测单词。</p>
<hr>
<h2 id="10-Exploring-Visual-Relationship-for-Image-Captioning"><a href="#10-Exploring-Visual-Relationship-for-Image-Captioning" class="headerlink" title="10. Exploring Visual Relationship for Image Captioning"></a>10. Exploring Visual Relationship for Image Captioning</h2><p>主要研究人员主页：<a href="http://tingyao.deepfun.club/、https://scholar.google.com.hk/citations?user=7Yq4wf4AAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener">http://tingyao.deepfun.club/、https://scholar.google.com.hk/citations?user=7Yq4wf4AAAAJ&amp;hl=zh-CN</a></p>
<h3 id="论文概述-5"><a href="#论文概述-5" class="headerlink" title="论文概述"></a>论文概述</h3><p>在其他解决图像标注的方法都没有关注图像中对象之间的关系，但是我们有理由相信，对对象之间的关系进行建模可以帮助模型更好地描述图像。因此，本文使用了一种全新的设计——在基于注意力的encoder-decoder框架下探索图像标注问题的对象之间的联系。</p>
<p>具体来说，本文提出了<strong>图形卷积网络和长短期记忆架构(GCN-LSTM)</strong>，它能将语义和空间对象关系集成到图像编码器中——基于检测到的对象的空间联系和语义联系来建图，然后通过GCN，利用图的结构来提炼每个区域代表的对象的表示。通过学习得到的区域级的特征，GCN-LSTM用基于LSTM的标注框架和注意机制来生成描述。</p>
<p><img src="16.png" alt="image-20181118104933285"></p>
<p>上图是对本文提出的GCN-LSTM的一个概述。图像首先通过Faster R-CNN来检测到一组显著的图像区域。然后，在这些区域上构建<strong>语义/空间有向图(semantic/spatial graph with directional edges)</strong>，其中顶点表示区域、边缘表示区域间的语义/空间关系。接下来，利用GCN在结构化语义/空间图中对具有视觉关系的区域进行上下文编码。最后，将来自每种图的学习的关系感知区域级特征反馈到一个单独的Attention LSTM解码器中来产生句子。 </p>
<hr>
<h2 id="11-Recurrent-Fusion-Network-for-Image-Captioning"><a href="#11-Recurrent-Fusion-Network-for-Image-Captioning" class="headerlink" title="11. Recurrent Fusion Network for Image Captioning"></a>11. Recurrent Fusion Network for Image Captioning</h2><p>主要研究人员主页：<a href="https://dblp.org/pers/hd/j/Jiang:Wenhao、http://www.yugangjiang.info/" target="_blank" rel="noopener">https://dblp.org/pers/hd/j/Jiang:Wenhao、http://www.yugangjiang.info/</a></p>
<h3 id="论文概述-6"><a href="#论文概述-6" class="headerlink" title="论文概述"></a>论文概述</h3><p>在基于encoder-decoder框架下的图像标注解决方案中，在encoder模块都只用一个特定的CNN，如ResNet或Inception-X，最后生成的特征也只描述来自一个特定视点的图像内容，无法全面理解图像中的语义内容。在本文中，利用来自多个编码器的<strong>互补信息(complementary information)</strong>，提出了一种新的循环融合网络(RFNet)。在RFNet中，在编码器和解码器之间会一个插入循环融合的过程。该模型的融合过程可以利用多个图像编码器之间输出之间的相互作用，为解码器生成新的<strong>压缩和语义表示(compact and informative representations)</strong>。</p>
<p><img src="17.png" alt="image-20181118110654565"></p>
<p>上图是RFNet的结构，使用了多个CNN作为编码器，并在编码器之后插入循环融合程序，让解码器能有更好的表示效果。RFNet的循环融合过程包括两个阶段，每个阶段可以被视为一个特殊的RNN。 在第一阶段，通过吸收来自其他表示的互补信息，将每个图像表示压缩成一组<strong>思想向量(thought vectors)</strong>。 然后在第二阶段将生成的思想向量集压缩成另一组思想向量，其将用作解码器的注意力模块的输入。</p>
<hr>
<h2 id="12-Show-Tell-and-Discriminate-Image-Captioning-by-Self-retrieval-with-Partially-Labeled-Data"><a href="#12-Show-Tell-and-Discriminate-Image-Captioning-by-Self-retrieval-with-Partially-Labeled-Data" class="headerlink" title="12. Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data"></a>12. Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</h2><p>主要研究人员主页：<a href="https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ&amp;hl=en、http://www.ee.cuhk.edu.hk/~hsli/、http://gr.xjtu.edu.cn/web/dapengchen" target="_blank" rel="noopener">https://scholar.google.com.hk/citations?user=4YL23GMAAAAJ&amp;hl=en、http://www.ee.cuhk.edu.hk/~hsli/、http://gr.xjtu.edu.cn/web/dapengchen</a></p>
<h3 id="论文概述-7"><a href="#论文概述-7" class="headerlink" title="论文概述"></a>论文概述</h3><p>本论文认为，尽管很多研究人员在图像标注上做出了巨大努力，但为图像生成<strong>区别性描述(discriminative captions)</strong>仍然是<strong>不平凡的(non-trivial)</strong>——大多数传统方法模仿语言的结构模式，最后往往会忽略每个图像的独特方面，陷入对常用短语或句子的重复中：</p>
<p><img src="18.png" alt="image-20181118110319254"></p>
<p>从上图可见，模型生成的标注非常普通且模块化。</p>
<p>因此本文提出了一个包含自我检索模块的图像标注模型，自我检索模块鼓励模型更多地产生区别性的描述。这样有两个优势：</p>
<ol>
<li>自我检索模块可以度量和评估模型生成的表述，以确保生成的描述的质量</li>
<li>生成的描述和图像之间的对应关系会很自然地包含进<strong>生成过程(generation process)</strong>中，而不需要人工进行标注。因此本论文的方法可以利用大量未标记的图像来提高模型生成描述的性能而无需额外的标注。</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/图像标注/" rel="tag"># 图像标注</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/25/show-and-tell/" rel="prev" title="Show and Tell:A Neural Image Caption Generator">
                Show and Tell:A Neural Image Caption Generator <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/panda.jpg" alt="Citehtap">
            
              <p class="site-author-name" itemprop="name">Citehtap</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-总结"><span class="nav-number">1.</span> <span class="nav-text">0.总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#任务描述"><span class="nav-number">1.1.</span> <span class="nav-text">任务描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#研究思路"><span class="nav-number">1.2.</span> <span class="nav-text">研究思路</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-基于LSTM的方法"><span class="nav-number">1.2.1.</span> <span class="nav-text">1. 基于LSTM的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-基于CNN的方法"><span class="nav-number">1.2.2.</span> <span class="nav-text">2. 基于CNN的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-注意力机制"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.注意力机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用数据集"><span class="nav-number">1.3.</span> <span class="nav-text">常用数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评价标准"><span class="nav-number">1.4.</span> <span class="nav-text">评价标准</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#下一阶段的学习"><span class="nav-number">1.5.</span> <span class="nav-text">下一阶段的学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Show-and-Tell-A-Neural-Image-Caption-Generator"><span class="nav-number">2.</span> <span class="nav-text">1. Show and Tell: A Neural Image Caption Generator</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型"><span class="nav-number">2.1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果"><span class="nav-number">2.2.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#问题与挑战"><span class="nav-number">2.3.</span> <span class="nav-text">问题与挑战</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention"><span class="nav-number">3.</span> <span class="nav-text">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型-1"><span class="nav-number">3.1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#软与硬"><span class="nav-number">3.2.</span> <span class="nav-text">软与硬</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练过程"><span class="nav-number">3.3.</span> <span class="nav-text">训练过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果-1"><span class="nav-number">3.4.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#改进与挑战"><span class="nav-number">3.5.</span> <span class="nav-text">改进与挑战</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Convolutional-Image-Captioning"><span class="nav-number">4.</span> <span class="nav-text">3. Convolutional Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型-2"><span class="nav-number">4.1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#体系结构"><span class="nav-number">4.2.</span> <span class="nav-text">体系结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#注意力机制"><span class="nav-number">4.3.</span> <span class="nav-text">注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果-2"><span class="nav-number">4.4.</span> <span class="nav-text">结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-What-Value-Do-Explicit-High-Level-Concepts-Have-in-Vision-to-Language-Problems"><span class="nav-number">5.</span> <span class="nav-text">4. What Value Do Explicit High Level Concepts Have in Vision to Language Problems?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#以属性为基础的V2L模型-An-Attribute-based-V2L-Model"><span class="nav-number">5.1.</span> <span class="nav-text">以属性为基础的V2L模型(An Attribute-based V2L Model)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#属性预测器"><span class="nav-number">5.2.</span> <span class="nav-text">属性预测器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语言生成器"><span class="nav-number">5.3.</span> <span class="nav-text">语言生成器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果-3"><span class="nav-number">5.4.</span> <span class="nav-text">结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Learning-to-Evaluate-Image-Captioning"><span class="nav-number">6.</span> <span class="nav-text">5. Learning to Evaluate Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述"><span class="nav-number">6.1.</span> <span class="nav-text">论文概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评价指标的评价标准"><span class="nav-number">6.2.</span> <span class="nav-text">评价指标的评价标准</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Rethinking-the-Form-of-Latent-States-in-Image-Captioning"><span class="nav-number">7.</span> <span class="nav-text">6. Rethinking the Form of Latent States in Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-1"><span class="nav-number">7.1.</span> <span class="nav-text">论文概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一维的弊端"><span class="nav-number">7.2.</span> <span class="nav-text">一维的弊端</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二维的优势"><span class="nav-number">7.3.</span> <span class="nav-text">二维的优势</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Unpaired-Image-Captioning-by-Language-Pivoting"><span class="nav-number">8.</span> <span class="nav-text">7. Unpaired Image Captioning by Language Pivoting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-2"><span class="nav-number">8.1.</span> <span class="nav-text">论文概述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Boosted-Attention-Leveraging-Human-Attention-for-Image-Captioning"><span class="nav-number">9.</span> <span class="nav-text">8. Boosted Attention: Leveraging Human Attention for Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-3"><span class="nav-number">9.1.</span> <span class="nav-text">论文概述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-“Factual”-or-“Emotional”-Stylized-Image-Captioning-with-Adaptive-Learning-and-Attention"><span class="nav-number">10.</span> <span class="nav-text">9. “Factual” or “Emotional”: Stylized Image Captioning with Adaptive Learning and Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-4"><span class="nav-number">10.1.</span> <span class="nav-text">论文概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#style-factual-LSTM"><span class="nav-number">10.2.</span> <span class="nav-text">style-factual LSTM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Exploring-Visual-Relationship-for-Image-Captioning"><span class="nav-number">11.</span> <span class="nav-text">10. Exploring Visual Relationship for Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-5"><span class="nav-number">11.1.</span> <span class="nav-text">论文概述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Recurrent-Fusion-Network-for-Image-Captioning"><span class="nav-number">12.</span> <span class="nav-text">11. Recurrent Fusion Network for Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-6"><span class="nav-number">12.1.</span> <span class="nav-text">论文概述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-Show-Tell-and-Discriminate-Image-Captioning-by-Self-retrieval-with-Partially-Labeled-Data"><span class="nav-number">13.</span> <span class="nav-text">12. Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文概述-7"><span class="nav-number">13.1.</span> <span class="nav-text">论文概述</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Citehtap</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
